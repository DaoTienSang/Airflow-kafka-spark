from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_json, struct, when, lit, expr, collect_list, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType, TimestampType
import pandas as pd
import numpy as np
from datetime import datetime
import time

# Kh·ªüi t·∫°o SparkSession v·ªõi timeout v√† tham s·ªë ƒë·ªÉ tƒÉng t√≠nh ·ªïn ƒë·ªãnh
spark = SparkSession.builder \
    .appName("Stock Analysis") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
    .config("spark.network.timeout", "800s") \
    .config("spark.executor.heartbeatInterval", "120s") \
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.memory.fraction", "0.8") \
    .getOrCreate()

# Log level
spark.sparkContext.setLogLevel("WARN")

print("Spark session created successfully")

# ƒê·ªãnh nghƒ©a schema cho d·ªØ li·ªáu JSON t·ª´ Kafka
schema = StructType([
    StructField("symbol", StringType()),
    StructField("current_price", DoubleType()),
    StructField("historical_data", ArrayType(
        StructType([
            StructField("time", StringType()),
            StructField("open", DoubleType()),
            StructField("high", DoubleType()),
            StructField("low", DoubleType()),
            StructField("close", DoubleType()),
            StructField("volume", DoubleType())
        ])
    ))
])

# D√πng ƒë·ªãa ch·ªâ network c·ªßa container Kafka trong Docker
kafka_bootstrap_servers = "airflow-kafka-1:9092"

# ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", "stock-history-topic") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 20000) \
    .option("failOnDataLoss", "false") \
    .load()

# Parse JSON t·ª´ Kafka
parsed_df = kafka_df \
    .selectExpr("CAST(value AS STRING) as json_data") \
    .select(from_json(col("json_data"), schema).alias("data")) \
    .select("data.*")

# Explode array ƒë·ªÉ x·ª≠ l√Ω t·ª´ng record l·ªãch s·ª≠
exploded_df = parsed_df \
    .select(
        col("symbol"),
        col("current_price"),
        expr("explode(historical_data) as history")
    ) \
    .select(
        col("symbol"),
        col("current_price"),
        col("history.time").alias("time"),
        col("history.open").alias("open"),
        col("history.high").alias("high"),
        col("history.low").alias("low"),
        col("history.close").alias("close"),
        col("history.volume").alias("volume")
    )

# Chuy·ªÉn timestamp string th√†nh timestamp th·ª±c v√† chuy·ªÉn th√†nh string date
# ƒê·ªÉ tr√°nh l·ªói datetime64 khi chuy·ªÉn sang pandas
exploded_df = exploded_df \
    .withColumn("timestamp", to_timestamp(col("time"), "yyyy-MM-dd")) \
    .withColumn("date_str", expr("date_format(timestamp, 'yyyy-MM-dd')"))

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u tr·ªØ SparkContext state
spark_active = True

# X·ª≠ l√Ω d·ªØ li·ªáu theo batch thay v√¨ s·ª≠ d·ª•ng window functions
def process_batch(batch_df, batch_id):
    global spark_active
    
    try:
        # Ki·ªÉm tra SparkContext c√≤n ho·∫°t ƒë·ªông kh√¥ng
        if not spark_active or spark.sparkContext._jsc.sc().isStopped():
            print(f"SparkContext ƒë√£ b·ªã shutdown, kh√¥ng th·ªÉ x·ª≠ l√Ω batch {batch_id}")
            return
            
        # Ki·ªÉm tra batch c√≥ d·ªØ li·ªáu kh√¥ng
        try:
            if batch_df.isEmpty():
                print(f"Batch {batch_id} kh√¥ng c√≥ d·ªØ li·ªáu, b·ªè qua")
                return
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ ki·ªÉm tra batch r·ªóng: {e}")
            # N·∫øu kh√¥ng th·ªÉ ki·ªÉm tra isEmpty, gi·∫£ ƒë·ªãnh l√† c√≥ d·ªØ li·ªáu v√† ti·∫øp t·ª•c
        
        # X√≥a c·ªôt timestamp tr∆∞·ªõc khi chuy·ªÉn sang Pandas ƒë·ªÉ tr√°nh l·ªói
        # V√† s·ª≠ d·ª•ng date_str thay th·∫ø
        batch_df_no_ts = batch_df.drop("timestamp")
        
        # L·∫•y danh s√°ch c√°c symbol ri√™ng bi·ªát trong batch n√†y ƒë·ªÉ x·ª≠ l√Ω
        symbols_in_batch = batch_df_no_ts.select("symbol").distinct().rdd.flatMap(lambda x: x).collect()
        print(f"üîç Batch {batch_id} c√≥ {len(symbols_in_batch)} m√£ c·ªï phi·∫øu: {symbols_in_batch}")
        
        # N·∫øu batch n√†y kh√¥ng c√≥ d·ªØ li·ªáu nhi·ªÅu, ƒë·ª£i th√™m th·ªùi gian ƒë·ªÉ t√≠ch l≈©y d·ªØ li·ªáu
        if len(symbols_in_batch) < 10:
            print(f"Batch {batch_id} ch·ªâ c√≥ {len(symbols_in_batch)} m√£, ƒë·ª£i th√™m d·ªØ li·ªáu...")
            if batch_id < 3:  # Ch·ªâ ƒë·ª£i th√™m trong c√°c batch ƒë·∫ßu ti√™n
                time.sleep(60)  # ƒê·ª£i th√™m 60 gi√¢y
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng d·ªØ li·ªáu n·∫øu qu√° l·ªõn ƒë·ªÉ tr√°nh qu√° t·∫£i
        count = batch_df_no_ts.count()
        print(f"Batch {batch_id} c√≥ {count} records")
        
        if count > 20000:
            print(f"Batch {batch_id} qu√° l·ªõn ({count} records), ph√¢n chia th√†nh c√°c batch nh·ªè h∆°n")
            # Ph√¢n chia th√†nh nhi·ªÅu batch nh·ªè h∆°n
            batches = batch_df_no_ts.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2])
            for i, small_batch in enumerate(batches):
                try:
                    process_small_batch(small_batch, f"{batch_id}_{i}")
                except Exception as e:
                    print(f"L·ªói khi x·ª≠ l√Ω sub-batch {batch_id}_{i}: {e}")
        else:
            # X·ª≠ l√Ω batch nh·ªè
            process_small_batch(batch_df_no_ts, batch_id)
    
    except Exception as e:
        import traceback
        print(f"L·ªói trong process_batch {batch_id}: {e}")
        print(traceback.format_exc())

def process_small_batch(batch_df, batch_id):
    """X·ª≠ l√Ω m·ªôt batch nh·ªè d·ªØ li·ªáu"""
    
    if spark.sparkContext._jsc.sc().isStopped():
        print(f"SparkContext ƒë√£ b·ªã shutdown, kh√¥ng th·ªÉ x·ª≠ l√Ω batch {batch_id}")
        return
        
    try:
        # Chuy·ªÉn sang Pandas ƒë·ªÉ d·ªÖ t√≠nh to√°n c√°c ch·ªâ s·ªë k·ªπ thu·∫≠t
        batch_pd = batch_df.toPandas()
        
        if batch_pd.empty:
            print(f"Batch {batch_id} kh√¥ng c√≥ d·ªØ li·ªáu sau khi chuy·ªÉn sang pandas")
            return
        
        # Chuy·ªÉn date_str th√†nh datetime n·∫øu c·∫ßn
        batch_pd['date'] = pd.to_datetime(batch_pd['date_str'])
        
        # Nh√≥m theo symbol v√† s·∫Øp x·∫øp theo th·ªùi gian
        symbols = batch_pd['symbol'].unique()
        result_dfs = []
        
        for symbol in symbols:
            # L·ªçc d·ªØ li·ªáu cho m·ªói m√£ c·ªï phi·∫øu
            symbol_df = batch_pd[batch_pd['symbol'] == symbol].sort_values('date')
            
            # N·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu th√¨ t√≠nh c√°c ch·ªâ s·ªë
            if len(symbol_df) > 0:
                # T√≠nh MA5, MA20
                symbol_df['ma5'] = symbol_df['close'].rolling(window=5).mean()
                symbol_df['ma20'] = symbol_df['close'].rolling(window=20).mean()
                
                # T√≠nh RSI
                delta = symbol_df['close'].diff()
                gain = delta.where(delta > 0, 0).rolling(window=14).mean()
                loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
                
                # X·ª≠ l√Ω tr√°nh chia cho 0
                rs = pd.Series(np.where(loss == 0, 0, gain / loss), index=loss.index)
                symbol_df['rsi'] = 100 - (100 / (1 + rs))
                
                # T√≠nh MACD
                ema12 = symbol_df['close'].ewm(span=12, adjust=False).mean()
                ema26 = symbol_df['close'].ewm(span=26, adjust=False).mean()
                
                symbol_df['macd_line'] = ema12 - ema26
                symbol_df['macd_signal'] = symbol_df['macd_line'].ewm(span=9, adjust=False).mean()
                symbol_df['macd_histogram'] = symbol_df['macd_line'] - symbol_df['macd_signal']
                
                # ƒê∆∞a ra ƒë·ªÅ xu·∫•t d·ª±a tr√™n c√°c ch·ªâ s·ªë
                conditions = [
                    # BUY: Xu h∆∞·ªõng tƒÉng (MA5 > MA20), RSI ch∆∞a qu√° mua, MACD t√≠ch c·ª±c
                    (symbol_df['ma5'] > symbol_df['ma20']) & 
                    (symbol_df['rsi'] < 70) & 
                    (symbol_df['macd_line'] > symbol_df['macd_signal']),
                    
                    # HOLD/SELL: Xu h∆∞·ªõng tƒÉng nh∆∞ng RSI qu√° mua
                    (symbol_df['ma5'] > symbol_df['ma20']) & 
                    (symbol_df['rsi'] >= 70),
                    
                    # WATCH/BUY: Xu h∆∞·ªõng gi·∫£m nh∆∞ng RSI qu√° b√°n v√† MACD t√≠ch c·ª±c
                    (symbol_df['ma5'] < symbol_df['ma20']) & 
                    (symbol_df['rsi'] <= 30) & 
                    (symbol_df['macd_line'] > symbol_df['macd_signal']),
                    
                    # SELL/AVOID: Xu h∆∞·ªõng gi·∫£m v√† MACD ti√™u c·ª±c
                    (symbol_df['ma5'] < symbol_df['ma20']) & 
                    (symbol_df['macd_line'] <= symbol_df['macd_signal'])
                ]
                
                choices = ['BUY', 'HOLD/SELL', 'WATCH/BUY', 'SELL/AVOID']
                symbol_df['suggestion'] = np.select(conditions, choices, default='HOLD')
                
                # Th√™m l√Ω do
                reasons = [
                    "Xu h∆∞·ªõng tƒÉng (MA5 > MA20). RSI ch∆∞a qu√° mua. MACD t√≠ch c·ª±c (MACD > Signal). ƒê·ªÅ xu·∫•t: MUA - Xu h∆∞·ªõng tƒÉng, RSI ch∆∞a qu√° mua, MACD t√≠ch c·ª±c.",
                    "Xu h∆∞·ªõng tƒÉng (MA5 > MA20). Qu√° mua (RSI > 70). ƒê·ªÅ xu·∫•t: C√ÇN NH·∫ÆC B√ÅN - Th·ªã tr∆∞·ªùng c√≥ d·∫•u hi·ªáu qu√° mua.",
                    "Xu h∆∞·ªõng gi·∫£m (MA5 < MA20). Qu√° b√°n (RSI < 30). MACD t√≠ch c·ª±c (MACD > Signal). ƒê·ªÅ xu·∫•t: THEO D√ïI/MUA - Th·ªã tr∆∞·ªùng ƒëang qu√° b√°n, c√≥ d·∫•u hi·ªáu ƒë·∫£o chi·ªÅu.",
                    "Xu h∆∞·ªõng gi·∫£m (MA5 < MA20). MACD ti√™u c·ª±c (MACD < Signal). ƒê·ªÅ xu·∫•t: B√ÅN/TR√ÅNH - Xu h∆∞·ªõng gi·∫£m, MACD ti√™u c·ª±c."
                ]
                symbol_df['reason'] = np.select(conditions, reasons, default="Xu h∆∞·ªõng trung t√≠nh. ƒê·ªÅ xu·∫•t: GI·ªÆ - Ch·ªù t√≠n hi·ªáu r√µ r√†ng h∆°n.")
                
                # Th√™m v√†o k·∫øt qu·∫£
                result_dfs.append(symbol_df)
        
        # K·∫øt h·ª£p t·∫•t c·∫£ k·∫øt qu·∫£
        if result_dfs:
            result_df = pd.concat(result_dfs)
            
            # Ki·ªÉm tra l·∫°i SparkContext tr∆∞·ªõc khi ti·∫øp t·ª•c
            if spark.sparkContext._jsc.sc().isStopped():
                print(f"SparkContext ƒë√£ b·ªã shutdown, kh√¥ng th·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω batch {batch_id}")
                global spark_active
                spark_active = False
                return
                
            # Chuy·ªÉn Pandas DataFrame tr·ªü l·∫°i Spark DataFrame
            processed_df = spark.createDataFrame(result_df)
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ ghi v√†o Kafka
            output_df = processed_df.select(
                col("symbol"),
                col("time"),
                col("date_str"),
                col("open"),
                col("high"),
                col("low"),
                col("close"),
                col("volume"),
                col("current_price"),
                col("ma5"),
                col("ma20"),
                col("rsi"),
                col("macd_line"),
                col("macd_signal"),
                col("macd_histogram"),
                col("suggestion"),
                col("reason")
            ).withColumn("value", to_json(struct(
                col("symbol"), col("time"), col("open"), col("high"), col("low"), 
                col("close"), col("volume"), col("current_price"),
                col("ma5"), col("ma20"), col("rsi"), 
                col("macd_line"), col("macd_signal"), col("macd_histogram"),
                col("suggestion"), col("reason")
            )))
            
            # Hi·ªÉn th·ªã m·ªôt s·ªë d·ªØ li·ªáu m·∫´u ƒë·ªÉ debug
            try:
                output_df.select("symbol", "time", "close", "ma5", "ma20", "rsi", "macd_line", "suggestion").show(5, False)
            except Exception as e:
                print(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u: {e}")
            
            # Ghi d·ªØ li·ªáu v√†o Kafka
            try:
                output_df.selectExpr("symbol AS key", "value") \
                    .write \
                    .format("kafka") \
                    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
                    .option("topic", "stock-processed-topic") \
                    .save()
                
                print(f"ƒê√£ x·ª≠ l√Ω v√† ghi th√†nh c√¥ng {output_df.count()} records t·ª´ batch {batch_id}")
            except Exception as e:
                print(f"L·ªói khi ghi d·ªØ li·ªáu v√†o Kafka: {e}")
                
    except Exception as e:
        import traceback
        print(f"L·ªói trong process_small_batch {batch_id}: {e}")
        print(traceback.format_exc())

# Thi·∫øt l·∫≠p trigger ƒë·ªÉ x·ª≠ l√Ω theo t·ª´ng batch v·ªõi kho·∫£ng th·ªùi gian
query = exploded_df \
    .writeStream \
    .foreachBatch(process_batch) \
    .trigger(processingTime="10 minutes") \
    .outputMode("update") \
    .start()

print(f"B·∫Øt ƒë·∫ßu ghi d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o Kafka topic: stock-processed-topic qua {kafka_bootstrap_servers}")
print("ƒêang ch·ªù d·ªØ li·ªáu...")

# Th√™m x·ª≠ l√Ω ƒë·ªÉ ƒë√≥ng Spark gracefully khi c·∫ßn
import signal

def handle_sigterm(sig, frame):
    """X·ª≠ l√Ω t√≠n hi·ªáu SIGTERM ƒë·ªÉ ƒë√≥ng Spark gracefully"""
    print("Nh·∫≠n t√≠n hi·ªáu ƒë·ªÉ d·ª´ng ·ª©ng d·ª•ng, ƒë√≥ng Spark gracefully...")
    global spark_active
    spark_active = False
    if query is not None and query.isActive:
        query.stop()
    if not spark.sparkContext._jsc.sc().isStopped():
        spark.stop()
    print("ƒê√£ d·ª´ng Spark th√†nh c√¥ng")
    
# ƒêƒÉng k√Ω x·ª≠ l√Ω t√≠n hi·ªáu
signal.signal(signal.SIGTERM, handle_sigterm)
signal.signal(signal.SIGINT, handle_sigterm)

# Ch·ªù query k·∫øt th√∫c
try:
    query.awaitTermination()
except Exception as e:
    print(f"L·ªói trong qu√° tr√¨nh streaming: {e}")
    # Th·ª≠ restart query n·∫øu c√≥ l·ªói
    if not spark.sparkContext._jsc.sc().isStopped():
        print("Th·ª≠ restart query...")
        spark_active = True
        query = exploded_df \
            .writeStream \
            .foreachBatch(process_batch) \
            .trigger(processingTime="10 minutes") \
            .outputMode("update") \
            .start()
        query.awaitTermination()
finally:
    # ƒê·∫£m b·∫£o d·ª´ng gracefully khi k·∫øt th√∫c
    if not spark.sparkContext._jsc.sc().isStopped():
        spark.stop()
    print("·ª®ng d·ª•ng k·∫øt th√∫c.") 